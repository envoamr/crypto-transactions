# Crypto Transaction Analysis

This project, developed for the Data Engineering Zoomcamp 2025 Cohort, ingests cryptocurrency activity data in batches and produces a dashboard for analysis.

## Problem Statement

To fully understand the cryptocurrency market, we need to uncover the patterns connecting on-chain transaction activity and market trading. Similar to how a company's sales data can influence its stock price, this project analyzes the correlation between blockchain transaction data and exchange trading data to determine if a comparable relationship exists for cryptocurrencies.

## Disclaimer

The dataset is nearly 4GB, with around 32 million rows covering a three-month period. While youâ€™re welcome to reproduce this project, keep in mind that it comes with a cost. PySpark processes data day by day (spinning up a separate job for each day) rather than handling it all at once, which can quickly add up.

With that being said, if you do wish to reproduce this, you can follow along in the order the components of this pipeline appear in this documentation: Terraform, then Kestra, then PySpark, then the dashboard.

## Data Sources

### Market Trade Prices

[BITCOIN Historical Datasets 2018-2025 Binance API](https://www.kaggle.com/datasets/novandraanugrah/bitcoin-historical-datasets-2018-2024/)

### On-chain Transactions

[bigquery-public-data.crypto_bitcoin](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=crypto_bitcoin&page=dataset)

## Data Pipeline Overview

![image](img/pipeline-diagram.png)

Let's dive deeper into each component to understand the entire process. Feel free to follow along (after having read the [Disclaimer](#disclaimer)).

## Terraform (Infrastructure as Code)

Terraform files `main.tf` and `variables.tf` are used to create resources on Google Cloud Platform. Change the text `<YOUR PROJECTID>` in the variables file to your project ID.

### Resources created

Terraform creates the following resources:

1. Service Account that will be attached to the VM instance (that Kestra runs on) to give the neccessary permissions to interact with BigQuery and GCS.
2. IAM Roles for the service account to give permissions to read/write to GCS, read/write the project's and Google's Public BigQuery datasets, submit jobs to Dataproc serverless (for PySpark).
3. Firewall rule for resources with `kestra-vm` label to allow your computer's public IP address to connect to the VM instance and open the Kestra webpage.
    - **Important**: Replace `<YOUR IP ADDRESS>` in the `source_ranges` option to be your public IP address (you can go to [https://api.ipify.org/](https://api.ipify.org/)). This is better than just using `0.0.0.0/0` as the latter allows any IP address with the right credentials to connect to Kestra, rather than restricting it to only your computer.
4. VM Compute Instance `e2-medium` with 4GB ram and 20GB disk space labelled as `kestra-vm`. This is where Kestra, the workflow orchestrator, will be installed and run the daily pipeline.
5. GCS bucket to represent the data lake used to store the raw data, as well upload the local PySpark script and Jar files to the bucket, which are used in a Pyspark job.
6. BigQuery dataset, used to store fact and dimension tables. It also runs a SQL query to create two tables, `Fact_Transactions` and `Dim_MarketPrice`.

![image](img/tf-graph.png)

(graph generated by installing [Graphviz](https://graphviz.org/) running `terraform graph | dot -Tpng > graph.png`)

### Creating Resources (reproducibility)

Before running Terraform, you need to authenticate to GCP:
- Download [gcloud](https://cloud.google.com/sdk/docs/install)
- Set up the GCP project using `gcloud init`
- Sign into GCP using `gcloud auth application-default login`

Then, deploy the resources onto GCP using:
- `terraform plan` to view the changes to be made
- `terraform apply` to apply these changes

## Kestra (Workflow Orchestration)

Kestra is used to run a series of steps to trigger the data pipeline and complete it from start to finish, everyday at 3am.

Since Kestra lives in the VM Compute Instance (that Terraform created), follow these steps:

- Navigate to the VM Instances page on GCP Console and SSH into the virtual machine that Terraform created
- Download Docker and Docker Compose for Ubuntu
- Upload the `docker-compose.yml` file onto the VM, either by cloning this repository, copy & pasting into a text file using `nano`/`vim`, or some other way. Change the text `<YOUR PASSWORD>` on line 47 in the docker compose file to something secretive, then start it using `docker-compose -d`.

Once Kestra starts, copy the contents of the [etl-pipeline-trigger.yaml](src/flows/etl-pipeline-trigger.yaml) into a new flow. The namespace (or group of flows) it will be under is `de-zoomacamp-crypto`. 

### Kestra Tasks

The [Kestra flow](src/flows/etl-pipeline-trigger.yaml) is composed of three groups of tasks:

The first two handle data ingestion:

1. If `extract_btc_prices` is `true`, then (a) runs a Python script (similar to [ingest-market-data.py](src/scripts/ingest-market-data.py)) that fetches today's market data from Kaggle's API and converts it to a Parquet file, (b) uploads the file to the GCS data lake.
2. If `extract_btc_transactions` is `true`, then uses BigQuery's `EXPORT DATA` query to export today's transaction data to the GCS data lake as a Parquet file.

![alt text](img/gcs.png)

The third group handles data cleaning and transformation:

3. If `submit_pyspark_job` is `true`, then submit a batch job to Dataproc Serverless with the file `gs://{{ kv('GCP_BUCKET_NAME') }}/scripts/pyspark-transform-crypto.py`, which is the same file that Terraform uploaded ([pyspark-transform-crypto.py](/src/scripts/pyspark-transform-crypto.py)). This script (more on it [later](#pyspark-data-transformations)) takes the files in the data lake, transforms them, and outputs them into BigQuery's data warehouse.

Overall, this is how the flow looks like:

![image](img/etl-pipeline-trigger1.png)

![image](img/etl-pipeline-trigger2.png)

![image](img/etl-pipeline-trigger3.png)

### Secrets (KV Store)

You may have come across variables like `kv('GCP_BUCKET_NAME')`. These represent key-value pairs stored in the KV store. Make sure to populate them appropriately in Kestra's KV store.
- `GCP_PROJECT_ID` The project ID found on the GCP console or through `gcloud`
- `GCP_DATASET`: The BigQuery dataset name as created in Terraform
- `GCP_BUCKET_NAME`: The GCS bucket name as created in Terraform

![kestra's KV store](img/kv-store.png)

### Backfill (reproducibility)

To reproduce the data as in the project, go to the Triggers section of the flow and backfill from January 1, 2025, until today's date (**important**, see [Disclaimer](#disclaimer)). During the development of this project, it took around 6 minutes for each day, which includes a 2 minute delay since PySpark jobs can't run right after the other or you might a `PERMISSION_DENIED: Insufficient project quota to satisfy request: resource` error. This equates to ~8 hours in total for the backfill. After that, it should run daily at 3am EST as long as the VM instance and the docker containers are still running. Although this (i.e. 8 hours) backfill takes longer than usual, it is meant to be a one time thing at the start, so it is understandable.

## PySpark (Data Transformations)

PySpark was used to process the raw data by cleaning and transforming it into fact and dimension tables. The PySpark jobs were submitted to Google Dataproc Serverless instead of a Dataproc Cluster, as the job only takes 1-2 minutes to run daily. Keeping a cluster running 24/7 or managing cluster startup and shutdown in Kestra would add unnecessary complexity for such a straightforward task.

The script is dynamic and takes 5 arguments that are required whenever a job is to be submitted:
- --input_date=YYYY-MM-DD
- --projectid={{ kv('GCP_PROJECT_ID') }}
- --bq_dataset={{ kv('GCP_DATASET') }}
- --bucket={{ kv('GCP_BUCKET_NAME') }}
- --frequency={{ render(vars.btc_prices_frequency) }}

### Partitioning/Clustering

Both tables, `Fact_Transactions` and `Dim_MarketPrice`, are updated daily and include time-based data, meaning they are partitioned by month. As of now, the dataset spans 3 months, resulting in 3 partitions. This partitioning can reduce data processing time by limiting queries to the relevant partitions.

Additionally, both tables include cryptocurrency data. Currently, the dataset contains only Bitcoin, but the tables are clustered by cryptocurrency. This clustering ensures that related data is grouped together, further optimizing processing efficiency.

### Run (reproducibility)

The Python script used for the PySpark job is automatically uploaded from this repo to the GCS bucket during Terraform, and is automatically referenced by Kestra when Kestra is submitting a PySpark job to Dataproc. So as long as both are set up correctly as per the documentation, no intervention should be needed here.

## Looker Studio (Dashboard)

To put it all together, a dashboard was created. The dashboard consists of 2 visuals showing the distrubution of categorical data and 2 visuals showing the distribution of the data across a temporal line.

[Link to dashboard](https://lookerstudio.google.com/reporting/674f8151-8f82-4d0e-8ef9-bc6f342bfd4c)

![image](img/dashboard.png)

## Conclusion

To revisit the goal of this project, which is to assess the correlation between transaction data and cryptocurrency market data, two findings were observed:
1. There doesn't appear to be a consistent relationship between the number of transactions made in a day and the number of market trades on the same day. For example, on January 24, February 6, and March 2, there were spikes in market trades without a corresponding spike in the number of transactions. Similarly, on February 10, February 26, and March 15, there were spikes in the number of transactions but no significant spike in market trades.
2. There does seem to be a relationship between the total transaction amount and the total market volume traded on a given day. Both saw spikes on January 24, February 6, and March 1. On the other hand, on January 15, January 29, and March 18, both the total transaction amount and the market volume saw slight dips in their totals and volume.
